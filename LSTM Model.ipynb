{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwoZcPLD3wq9"
   },
   "source": [
    "\n",
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6_TD_YP3xD4"
   },
   "outputs": [],
   "source": [
    "# Download necessary libraries\n",
    "# !pip install transformers\n",
    "\n",
    "# Importing necessary libraries \n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from os import name\n",
    "gc.collect()\n",
    "\n",
    "# Model Creation and testing\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules import padding\n",
    "from torch.nn.modules.activation import Sigmoid\n",
    "\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# For GPU specific run\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xd2eEu2NSwH4",
    "outputId": "20e81920-c47f-4ccd-ee4e-23b72fa5b358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 125341\n"
     ]
    }
   ],
   "source": [
    "# Constructing vocabulary using Bag-of-words\n",
    "def construct_vocabulary(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    data.dropna(inplace=True)\n",
    "    sentences = \" \".join(data[\"new_sentence\"].values).lower()\n",
    "    unique_word = list(set(sentences.split()))\n",
    "    print(f\"Vocabulary: {len(unique_word)}\")\n",
    "    \n",
    "    def store_pickle(data, name):\n",
    "        pickle.dump(data,open(name, \"wb\"))\n",
    "    # storing the dataset:\n",
    "    store_pickle(unique_word, \"data/unique_vocab.pickle\")\n",
    "\n",
    "# Split dataset into train and test Train size = 0.7, Test_size = 0.3\n",
    "def splittFunction(filename):\n",
    "    df = pd.read_csv(filename)[:20000]\n",
    "    df.dropna(inplace=True)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df[\"new_sentence\"], df[\"Labels\"], test_size=0.3, stratify=df[\"Labels\"])\n",
    "    train = pd.concat([train_x, train_y], axis=1)\n",
    "    test  = pd.concat([test_x,  test_y], axis=1)\n",
    "    train.to_csv(\"data/train_LSTM.csv\", index=False)\n",
    "    test.to_csv(\"data/test_LSTM.csv\", index=False)\n",
    "\n",
    "\n",
    "splittFunction(\"data/new_dataset_LSTM.csv\")\n",
    "construct_vocabulary(\"data/train_LSTM.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B4yMHZL7SwBl"
   },
   "outputs": [],
   "source": [
    "# Creating sequential learning dataset for LSTM model training\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, pickleFile: str, filename: str   ) -> None:\n",
    "        super().__init__()\n",
    "        self.pickleFileName = pickleFile\n",
    "        self.filename   = filename\n",
    "        self.data = pd.read_csv(self.filename)\n",
    "        self.unique_vocab = self._loadPickle(self.pickleFileName)\n",
    "        self.pad_to_maxlength = 100\n",
    "        self.cnt = 0\n",
    "\n",
    "    def _loadPickle(self, filename):\n",
    "        return pickle.load(open(filename,\"rb\"))\n",
    "    \n",
    "    def _convertNumeric(self, data):\n",
    "        subsentence = []\n",
    "        splitted_word = data.split()\n",
    "        for word in splitted_word:\n",
    "            if word in self.unique_vocab:\n",
    "                subsentence.append(1)\n",
    "            else:\n",
    "                subsentence.append(0)\n",
    "            \n",
    "\n",
    "        if len(subsentence) < self.pad_to_maxlength:\n",
    "            remaining_length = self.pad_to_maxlength - len(subsentence)\n",
    "            subsentence = subsentence +[5]*remaining_length\n",
    "        elif len(subsentence) > self.pad_to_maxlength:\n",
    "            subsentence = subsentence[:self.pad_to_maxlength]\n",
    "\n",
    "        assert len(subsentence)  == self.pad_to_maxlength\n",
    "        return subsentence\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subdata =self.data.iloc[index]\n",
    "        sent, label = subdata[\"new_sentence\"], subdata[\"Labels\"]\n",
    "        numeric_sent = self._convertNumeric(sent)\n",
    "        self.cnt +=1\n",
    "        return {\n",
    "            \"input_ids\": numeric_sent,\n",
    "            \"label\":  label\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    X, Y = [], []\n",
    "    for dd in data:\n",
    "        input_ids = dd[\"input_ids\"]\n",
    "        label    = dd[\"label\"]\n",
    "        X.append(input_ids)\n",
    "        Y.append(label)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(X),\n",
    "        \"label\": torch.tensor(Y)\n",
    "    }\n",
    "\n",
    "train_dataset = dataset(\"data/unique_vocab.pickle\", \"data/train.csv\")\n",
    "# call the dataloader \n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rlUFMI4ISxWa"
   },
   "outputs": [],
   "source": [
    "# Creating LSTM model\n",
    "class MODEL(nn.Module):\n",
    "    def __init__(self,  num_of_embeddings: int, \\\n",
    "        output_class: int, embedding_dim: int = 128, padding_idx: int = 5, hidden_size: int= 128 ) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings = num_of_embeddings, embedding_dim = embedding_dim)\n",
    "        self.dropout = nn.Dropout()\n",
    "        #lstm layer \n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size=hidden_size)\n",
    "     \n",
    "        # dense layer \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(in_features= hidden_size, out_features= hidden_size)\n",
    "        self.output = nn.Linear(in_features= hidden_size, out_features= output_class)\n",
    "    \n",
    "    # forward layer loop \n",
    "    def forward(self,x):\n",
    "        embedding_output = self.embeddings(x)\n",
    "        embedding_output = self.dropout(embedding_output)\n",
    "        output, (hn, cn) = self.lstm(embedding_output)\n",
    "        output = torch.mean(output,dim=1)\n",
    "        # apply linear layer \n",
    "        output = self.relu(self.linear(output))\n",
    "        output = self.output(output)\n",
    "        return output, torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CduefLhfSxa0"
   },
   "outputs": [],
   "source": [
    "# Examine F1_score and accuracy of the model\n",
    "def f1Score(prediction, gt):\n",
    "    threshold =0.5\n",
    "    y_pred = [1 if out.item() > threshold else 0 for out in prediction]\n",
    "    return f1_score(y_pred, gt.detach().cpu().numpy().tolist())\n",
    "    \n",
    "def accuracy(prediction, gt):\n",
    "    threshold =0.5\n",
    "    y_pred = [1 if out.item() > threshold else 0 for out in prediction]\n",
    "    return accuracy_score(y_pred, gt.detach().cpu().numpy().tolist())\n",
    "\n",
    "# Model Training Loop\n",
    "def main_loop():\n",
    "    num_of_embeddings = len(pickle.load(open(\"data/unique_vocab.pickle\", \"rb\")))+1\n",
    "    output_class  = 1\n",
    "    model = MODEL(num_of_embeddings, output_class)\n",
    "    model.to(device)\n",
    "\n",
    "    # defining the loss function \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "\n",
    "    # call the dataset:\n",
    "    train_dataset = dataset(\"data/unique_vocab.pickle\", \"data/train_LSTM.csv\")\n",
    "    test_dataset  = dataset(\"data/unique_vocab.pickle\", \"data/test_LSTM.csv\")\n",
    "\n",
    "    # call the dataloader \n",
    "    trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "    testloader  = DataLoader(test_dataset, batch_size=16, shuffle=True, drop_last=True, collate_fn = collate_fn)\n",
    "\n",
    "    for ep in tqdm(range(10)): #100 iterations\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_f1 = []\n",
    "        train_acc = []\n",
    "        for idx, data in tqdm(enumerate(trainloader)):\n",
    "            optimizer.zero_grad()\n",
    "            X, Y = data[\"input_ids\"].to(device), data[\"label\"].to(device)\n",
    "            prediction, sigmoid_pred  = model(X.long())\n",
    "            loss = loss_fn(prediction.squeeze(), Y.float())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            train_f1.append(f1Score(sigmoid_pred, Y))\n",
    "            train_acc.append(accuracy(sigmoid_pred, Y))\n",
    "\n",
    "        if ep%2 == 0: # result after every 20 episodes\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                total_eval_loss = 0.0\n",
    "                test_f1 = []\n",
    "                test_acc = [] \n",
    "                for idx, data in tqdm(enumerate(testloader)):\n",
    "                    X, Y = data[\"input_ids\"].to(device), data[\"label\"].to(device)\n",
    "                    prediction, sigmoid_pred = model(X.long())\n",
    "                    loss = loss_fn(prediction.squeeze(), Y.float())\n",
    "                    total_eval_loss += loss.item()\n",
    "                    test_f1.append(f1Score(sigmoid_pred, Y))\n",
    "                    test_acc.append(accuracy(sigmoid_pred, Y))\n",
    "            \n",
    "            print(f\"Train Loss {total_loss/len(trainloader)} Test Loss {total_eval_loss/len(testloader)}\")\n",
    "            print(f\"Train Accuracy {np.array(train_acc).mean()} Test Accuracy {np.array(test_acc).mean()}\")\n",
    "            print(f\"Train F1 {np.array(train_f1).mean()}  Test F1 {np.array(test_f1).mean()}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict()  \n",
    "    }, \"lstm_classification.pth\")\n",
    "    \n",
    "main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Y33uKZuHnTb"
   },
   "outputs": [],
   "source": [
    "# ### Result\n",
    "# # LSTM Model iteration result/ summary with model f1_score and accuracy \n",
    "# \"\"\"\n",
    "# 10%|█         | 1/10 [1:09:09<10:22:24, 4149.40s/it]Train Loss 0.6916796985060669 Test Loss 0.690889467201803\n",
    "# Train Accuracy 0.5109738372093023 Test Accuracy 0.529891304347826\n",
    "# Train F1 0.013253995266739165  Test F1 0.1597046899356682\n",
    "\n",
    "#  30%|███       | 3/10 [3:05:54<7:15:13, 3730.53s/it]Train Loss 0.6891263084356175 Test Loss 0.6882554937316023\n",
    "# Train Accuracy 0.5587209302325581 Test Accuracy 0.5670855978260869\n",
    "# Train F1 0.2776830265385991  Test F1 0.30498007382997794\n",
    "\n",
    "# 50%|█████     | 5/10 [5:01:10<5:02:27, 3629.57s/it]Train Loss 0.6861516629540643 Test Loss 0.6854808743557205\n",
    "# Train Accuracy 0.5723837209302326 Test Accuracy 0.567764945652174\n",
    "# Train F1 0.34581146319766193  Test F1 0.32787235783054536\n",
    "\n",
    "# 70%|███████   | 7/10 [6:57:09<3:00:20, 3606.73s/it]Train Loss 0.683653856709946 Test Loss 0.6834396180575308\n",
    "# Train Accuracy 0.5734011627906976 Test Accuracy 0.5670855978260869\n",
    "# Train F1 0.34622344949886935  Test F1 0.3257361990098563\n",
    "\n",
    "# 90%|█████████  | 9/10 [7:48:09<1:08:28, 3582.77s/it]Train Loss 0.6812515148610264 Test Loss 0.6823069397065198\n",
    "# Train Accuracy 0.56940980473025118 Test Accuracy 0.5658443697202368\n",
    "# Train F1 0.34651397643247641  Test F1 0.3229318480364815\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "task22_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
